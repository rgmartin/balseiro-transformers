{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN26qoZR3hLe"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6oqGiIXvrMl"
   },
   "source": [
    "# Tutorial de Pytorch (basado en CS244N Stanford) \n",
    "\n",
    "\n",
    "Tendremos una introducción básica a `PyTorch` y trabajaremos en una tarea de NLP. Los siguientes recursos han sido utilizados para la confección de este notebook:\n",
    "\n",
    "* [\"Word Window Classification\" tutorial notebook]((https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/materials/ww_classifier.ipynb) by Matt Lamm, from Winter 2020 offering of CS224N\n",
    "* Official PyTorch Documentation on [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) by Soumith Chintala\n",
    "* PyTorch Tutorial Notebook, [Build Basic Generative Adversarial Networks (GANs) | Coursera](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans) by Sharon Zhou, offered on Coursera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2jvK3g_2zrK"
   },
   "source": [
    "# Por favor haz una copia en to Drive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gk1UKaNvrMv"
   },
   "source": [
    "## Introducción \n",
    "[PyTorch](https://pytorch.org/) es un framework de deep learning, una de las mas importantes junto a [TensorFlow](https://www.tensorflow.org/). La instalación puede ser realizada via Pip, como se describe [aquí](https://pytorch.org/). Empecemos importando Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:14:48.949326Z",
     "start_time": "2024-11-29T15:14:48.866100Z"
    },
    "id": "u0ukr7quvrMx"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Import pprint, module we use for making our print statements prettier\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import pprint, module we use for making our print statements prettier\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k10ZRdcBwDP3"
   },
   "source": [
    "Listo! Empecemos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLdSN9ZXvrM0"
   },
   "source": [
    "## Part 1: Tensors\n",
    "\n",
    "Los **Tensores** son la unidad básica de Pytorch. Cada tensor es una matriz multidimensional; por ejemplo, una imagen cuadrada de 256x256 podría ser representada por un tensor `3x256x256`, donde la primera dimensión representa el color. Veamos cómo crear un tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXD7cTLTh4oF",
    "outputId": "e8662dba-7c52-4324-a4f7-fd10792fcb91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "list_of_lists = [\n",
    "  [1, 2, 3],\n",
    "  [4, 5, 6],\n",
    "]\n",
    "print(list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2u7NOqWNJ7c",
    "outputId": "7be0d98b-cd5c-499d-a870-0016c0476ae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(list_of_lists)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VLgHhj0n3LM",
    "outputId": "361eaa7b-d240-4c79-fafd-4d3ba9991d3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Initializing a tensor\n",
    "data = torch.tensor([\n",
    "                     [0, 1],\n",
    "                     [2, 3],\n",
    "                     [4, 5]\n",
    "                    ])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i7vrR1_oO4I"
   },
   "source": [
    "Cada tensor tiene un **data type**: los principales data types que necesitarás son floats (`torch.float32`) e integers (`torch.int`). Puedes especificar el data type explícitamente cuando creas el tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7nMdqgMoLOa",
    "outputId": "684c26eb-017b-4c9c-b66f-8851568a35f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# Initializing a tensor with an explicit data type\n",
    "# Notice the dots after the numbers, which specify that they're floats\n",
    "data = torch.tensor([\n",
    "                     [0, 1],\n",
    "                     [2, 3],\n",
    "                     [4, 5]\n",
    "                    ], dtype=torch.float32)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4vr6EYbiOEJ",
    "outputId": "344bda95-1691-47b7-f367-3d17481c2fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1111, 1.0000],\n",
      "        [2.0000, 3.0000],\n",
      "        [4.0000, 5.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Initializing a tensor with an explicit data type\n",
    "# Notice the dots after the numbers, which specify that they're floats\n",
    "data = torch.tensor([\n",
    "                     [0.11111111, 1],\n",
    "                     [2, 3],\n",
    "                     [4, 5]\n",
    "                    ], dtype=torch.float32)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jnB8J23WiUTi",
    "outputId": "fae5e40f-2ca8-4ec5-a794-4dd66e516867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1111, 1.0000],\n",
      "        [2.0000, 3.0000],\n",
      "        [4.0000, 5.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Initializing a tensor with an explicit data type\n",
    "# Notice the dots after the numbers, which specify that they're floats\n",
    "data = torch.tensor([\n",
    "                     [0.11111111, 1],\n",
    "                     [2, 3],\n",
    "                     [4, 5]\n",
    "                    ])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También tenemos funciones auxiliares para crear tensores con formas y contenidos particulares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tn-N-z_qYj0",
    "outputId": "df692e12-9f8b-458b-b70f-8f4ee667c101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 5)  # a tensor of all zeros\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ue26M5Npqoe3",
    "outputId": "98b18695-a174-4b61-8545-a32fa1127ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(3, 4)   # a tensor of all ones\n",
    "print(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdvaujtvqokI",
    "outputId": "77123765-a189-4d8c-ac6b-e2e4a772717f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "rr = torch.arange(1, 10) # range from [1, 10)\n",
    "print(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lK3q3LRHipHB",
    "outputId": "23d9bdfd-5b9b-45c9-8048-fc469859d614"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDQ-v6AFiyco",
    "outputId": "5ea664d0-7968-4ebe-b824-01d025b927e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  8, 10, 12, 14, 16, 18])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFcjwshvi3pC",
    "outputId": "81bb1488-0c44-40c4-b381-89d5ffbdf921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A is tensor([[1, 2],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n",
      "B is tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "The product is tensor([[11, 14, 17, 20],\n",
      "        [17, 22, 27, 32],\n",
      "        [29, 38, 47, 56]])\n",
      "The other product is tensor([[11, 14, 17, 20],\n",
      "        [17, 22, 27, 32],\n",
      "        [29, 38, 47, 56]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [2, 3], [4, 5]])      # (3, 2)\n",
    "b = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])  # (2, 4)\n",
    "\n",
    "print(\"A is\", a)\n",
    "print(\"B is\", b)\n",
    "print(\"The product is\", a.matmul(b)) #(3, 4)\n",
    "print(\"The other product is\", a @ b) # +, -, *, @"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCS5z1lip9lq"
   },
   "source": [
    "La **shape** de una matriz (que puede ser accedida por `.shape`) se define como las dimensiones de la matriz. Aquí tienes algunos ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMptIpZkq0da",
    "outputId": "0466da76-e096-4814-8ae9-c3c000751db1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "matr_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(matr_2d.shape)\n",
    "print(matr_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWjD7WjPqC6t",
    "outputId": "6bd07e79-fb89-462c-df24-2e5b7e52d004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [-2,  5,  6,  9]],\n",
      "\n",
      "        [[ 5,  6,  7,  2],\n",
      "         [ 8,  9, 10,  4]],\n",
      "\n",
      "        [[-3,  2,  2,  1],\n",
      "         [ 4,  6,  5,  9]]])\n",
      "torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "matr_3d = torch.tensor([[[1, 2, 3, 4], [-2, 5, 6, 9]], [[5, 6, 7, 2], [8, 9, 10, 4]], [[-3, 2, 2, 1], [4, 6, 5, 9]]])\n",
    "print(matr_3d)\n",
    "print(matr_3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wKqP85rrF-P"
   },
   "source": [
    "**Reshaping**  tensores  puede ser usado para hacer operaciones en batches más fáciles, pero ten cuidado de que los datos se reconfigure en el orden que esperas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmUcqHYUrMu1",
    "outputId": "ba15f448-9355-4fd4-f773-a8eb32c577cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape is currently torch.Size([15])\n",
      "The contents are currently tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "\n",
      "After reshaping, the shape is currently torch.Size([5, 3])\n",
      "The contents are currently tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "rr = torch.arange(1, 16)\n",
    "print(\"The shape is currently\", rr.shape)\n",
    "print(\"The contents are currently\", rr)\n",
    "print()\n",
    "rr = rr.view(5, 3)\n",
    "print(\"After reshaping, the shape is currently\", rr.shape)\n",
    "print(\"The contents are currently\", rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaykBuhoou3M"
   },
   "source": [
    "También podemos convertir tensores a  **NumPy arrays**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppYiPnlko1Ci",
    "outputId": "328decae-69bb-4f8d-9248-c182868bec9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a torch.tensor tensor([[1, 0, 5]])\n",
      "This is a np.ndarray [[1 0 5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numpy.ndarray --> torch.Tensor:\n",
    "arr = np.array([[1, 0, 5]])\n",
    "data = torch.tensor(arr)\n",
    "print(\"This is a torch.tensor\", data)\n",
    "\n",
    "# torch.Tensor --> numpy.ndarray:\n",
    "new_arr = data.numpy()\n",
    "print(\"This is a np.ndarray\", new_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyv1l431q9yA"
   },
   "source": [
    "Una de las razones por las cuales usamos **tensors** es para las operaciones vectorizadas: operaciones que se pueden realizar en paralelo sobre una dimensión particular de un tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kas2MEFDsJWk",
    "outputId": "fcbd6032-cbe2-4d83-f605-a4244e69fe95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19., 20., 21.],\n",
      "        [22., 23., 24., 25., 26., 27., 28.],\n",
      "        [29., 30., 31., 32., 33., 34., 35.]])\n",
      "Taking the sum over rows:\n",
      "tensor([ 28.,  77., 126., 175., 224.])\n",
      "Taking thep sum over columns:\n",
      "tensor([ 75.,  80.,  85.,  90.,  95., 100., 105.])\n",
      "Taking the stdev over rows:\n",
      "tensor([2.1602, 2.1602, 2.1602, 2.1602, 2.1602])\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(1, 36, dtype=torch.float32).reshape(5, 7)\n",
    "print(\"Data is:\", data)\n",
    "\n",
    "# We can perform operations like *sum* over each row...\n",
    "print(\"Taking the sum over rows:\")\n",
    "print(data.sum(dim=1)) #(5,)\n",
    "\n",
    "# or over each column.\n",
    "print(\"Taking thep sum over columns:\")\n",
    "print(data.sum(dim=0)) #(7,)\n",
    "\n",
    "# Other operations are available:\n",
    "print(\"Taking the stdev over rows:\")\n",
    "print(data.std(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1ByKJL_WYWv",
    "outputId": "d96d9b26-9742-4d25-ce39-800014ef1751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.]]])\n",
      "tensor([5., 7., 9.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(1, 7, dtype=torch.float32).reshape(1, 2, 3)\n",
    "print(data)\n",
    "print(data.sum(dim=0).sum(dim=0))\n",
    "print(data.sum(dim=0).sum(dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPRy-xtuk2tK",
    "outputId": "90348fd4-9f07-4274-83d7-168d06288292"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ8MjWEMxOVk"
   },
   "source": [
    "### Quiz\n",
    "\n",
    "Escribe un código que cree un `torch.tensor` con el siguiente contenido:\n",
    "$\\begin{bmatrix} 1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\end{bmatrix}$\n",
    "\n",
    "Ahora calcula el promedio de cada fila (`.mean()`) y de cada columna.\n",
    "\n",
    "Cuál es la forma de los resultados?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BK0YInGkn3Xy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7BMktFFAkRA"
   },
   "source": [
    "**Indexing**\n",
    "\n",
    "Puedes acceder a elementos arbitrarios de un tensor usando el operador `[]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fRJN7ovWDsKV",
    "outputId": "9a207766-1b7e-4489-ef5f-e5d5dbe20d15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.],\n",
       "         [ 3.,  4.]],\n",
       "\n",
       "        [[ 5.,  6.],\n",
       "         [ 7.,  8.]],\n",
       "\n",
       "        [[ 9., 10.],\n",
       "         [11., 12.]]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an example tensor\n",
    "x = torch.Tensor([\n",
    "                  [[1, 2], [3, 4]],\n",
    "                  [[5, 6], [7, 8]],\n",
    "                  [[9, 10], [11, 12]]\n",
    "                 ])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M67ZiOF1Heyc",
    "outputId": "83a18ebb-cc54-4608-c96c-543fa636babc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guXKE7m8AX1K",
    "outputId": "edffdcfd-d250-4749-b31c-0908b2674529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the 0th element, which is the first row\n",
    "x[0] # Equivalent to x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zn4pW2rkmXuj",
    "outputId": "514236c4-7fc5-46cb-f84f-ff3be2d4e6a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.],\n",
       "        [ 5.,  6.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8m8EyVvES4-"
   },
   "source": [
    "También podemos indexar en múltiples dimensiones con `:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Z6GFUcuEL85",
    "outputId": "13011b02-d061-43b7-ef2b-cf188323c80c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 9.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top left element of each element in our tensor\n",
    "x[:, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRkMhiJNnWAt",
    "outputId": "3b93bd7f-373a-47c2-a67d-71315e5e4f8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.],\n",
       "         [ 3.,  4.]],\n",
       "\n",
       "        [[ 5.,  6.],\n",
       "         [ 7.,  8.]],\n",
       "\n",
       "        [[ 9., 10.],\n",
       "         [11., 12.]]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm8vc3nuXaEw"
   },
   "source": [
    "Podemos usar tensores para acceder a elementos arbitrarios de un tensor en cualquier dimensión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4xl6CW3RrEw",
    "outputId": "18672bf8-3d9c-4fb4-c967-7d26608cde2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2.],\n",
       "         [3., 4.]],\n",
       "\n",
       "        [[1., 2.],\n",
       "         [3., 4.]],\n",
       "\n",
       "        [[5., 6.],\n",
       "         [7., 8.]],\n",
       "\n",
       "        [[5., 6.],\n",
       "         [7., 8.]]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's access the 0th and 1st elements, each twice\n",
    "# same as stacking x[0], x[0], x[1], x[1]\n",
    "i = torch.tensor([0, 0, 1, 1])\n",
    "x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3QYZ8k7Wvqp",
    "outputId": "2fb10213-5441-4326-ddf3-6e5014922304"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's access the 0th elements of the 1st and 2nd elements\n",
    "\n",
    "i = torch.tensor([1, 2])\n",
    "j = torch.tensor([0])\n",
    "x[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAELXC--IHS7"
   },
   "source": [
    "Podemos obtener un valor escalar de un tensor con `item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BM-ZujN2IGaQ",
    "outputId": "ece51f0a-f1d2-435c-d75f-0496035a7d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NwxK7d_Ycgs",
    "outputId": "b960776c-df09-49ad-def1-e951c55c1d10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0, 0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGod5kHa6OOF"
   },
   "source": [
    "### Ejercicio:\n",
    "Escibe un código que cree un `torch.tensor` con el siguiente contenido:\n",
    "$\\begin{bmatrix} 1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\end{bmatrix}$\n",
    "\n",
    "Cómo obtienes la primera columna? La primera fila?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Re8xiL37eAja"
   },
   "source": [
    "## Autograd\n",
    "Pytorch es bien conocido por su característica de diferenciación automática. Podemos llamar al método `backward()` para pedirle a `PyTorch` que calcule los gradientes, que luego se almacenan en el atributo `grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-oEvBJHWfn8H",
    "outputId": "fa65c16c-8041-4ae0-a79d-7ffbd31e2e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create an example tensor\n",
    "# requires_grad parameter tells PyTorch to store gradients\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "# Print the gradient if it is calculated\n",
    "# Currently None since x is a scalar\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTJazZXkgthP",
    "outputId": "8730c813-ae43-4316-f84d-68d65ca0bd9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "# Calculating the gradient of y with respect to x\n",
    "y = x * x * 3 # 3x^2\n",
    "y.backward()\n",
    "pp.pprint(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Hqc2oM3iV6a"
   },
   "source": [
    "Ahora corramos backprop desde un tensor diferente para ver qué pasa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K--Az0Xiic_z",
    "outputId": "07e848bc-f87a-476f-fc57-44a74be50874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24.])\n"
     ]
    }
   ],
   "source": [
    "z = x * x * 3 # 3x^2\n",
    "z.backward()\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tIa4dw6Tmar",
    "outputId": "27935662-bf85-4421-9829-c43bf9d651a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "x.grad = None\n",
    "z = x * x * 3 # 3x^2\n",
    "z.backward()\n",
    "# y = x * x * 3\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbyr-P4rdJ9b",
    "outputId": "7ad721e9-ef65-4309-b765-c63f9402a814"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24.])\n"
     ]
    }
   ],
   "source": [
    "z = x * x * 3 # 3x^2\n",
    "z.backward()\n",
    "# y = x * x * 3\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpvRq_wUdM5y",
    "outputId": "34aaf32c-5ac9-4c19-8dfe-c34fe8f05f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36.])\n"
     ]
    }
   ],
   "source": [
    "z = x * x * 3 # 3x^2\n",
    "z.backward()\n",
    "# y = x * x * 3\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhjPkiE6i7ja"
   },
   "source": [
    "Podemos ver que los gradientes se acumulan. Cuando ejecutamos `backward()` en un tensor, los gradientes se acumulan en el atributo `grad`. Cuando queremos calcular los gradientes de un tensor desde cero, necesitamos reiniciar los gradientes con `zero_grad()`. De lo contrario, nuestros gradientes se acumularán en cada paso de backprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-9I0zZOKtXr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYLWqKIoaOyd"
   },
   "source": [
    "## Neural Network Module\n",
    "\n",
    "Hasta ahora hemos visto tensores, sus propiedades y operaciones básicas en tensores. Estos son especialmente útiles para familiarizarse si estamos construyendo las capas de nuestra red desde cero. Utilizaremos estos en la Tarea 2, pero en el futuro, utilizaremos bloques predefinidos en el módulo `torch.nn` de `PyTorch`. Luego juntaremos estos bloques para crear redes complejas. Comencemos importando este módulo con un alias para que no tengamos que escribir `torch` cada vez que lo usemos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUmrDpbhV4Tn"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joGvRWjEbak0"
   },
   "source": [
    "### **Linear Layer**\n",
    "Podemos usar `nn.Linear(H_in, H_out)` para crear una capa lineal. Esto tomará una matriz de dimensiones `(N, *, H_in)` y devolverá una matriz de dimensiones `(N, *, H_out)`. El `*` denota que podría haber un número arbitrario de dimensiones en medio. La capa lineal realiza la operación `Ax+b`, donde `A` y `b` se inicializan aleatoriamente. Si no queremos que la capa lineal aprenda los parámetros de sesgo, podemos inicializar nuestra capa con `bias=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XfnKI4-a5j9",
    "outputId": "ebce3d47-b720-4ff4-a279-83c2ab3513cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8810,  0.3769],\n",
       "         [-0.8810,  0.3769],\n",
       "         [-0.8810,  0.3769]],\n",
       "\n",
       "        [[-0.8810,  0.3769],\n",
       "         [-0.8810,  0.3769],\n",
       "         [-0.8810,  0.3769]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the inputs\n",
    "input = torch.ones(2,3,4)\n",
    "# N* H_in -> N*H_out\n",
    "\n",
    "\n",
    "# Make a linear layers transforming N,*,H_in dimensinal inputs to N,*,H_out\n",
    "# dimensional outputs\n",
    "linear = nn.Linear(4, 2)\n",
    "linear_output = linear(input)\n",
    "linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ao4XddjxeBVJ",
    "outputId": "088e2c85-511a-4508-c392-f38e38abf15f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10, 11, 2])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_9XKtAFYpdI",
    "outputId": "992db13a-f581-4cfd-f840-fa49778e696e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1443,  0.2130,  0.2116, -0.4267],\n",
       "         [-0.3379,  0.2243,  0.3289, -0.2484]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.4608,  0.0073], requires_grad=True)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear.parameters()) # Ax + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsyBaqzZppS8"
   },
   "outputs": [],
   "source": [
    "# Data of shape [batch_size, feature_dim] # 4\n",
    "# [batch_size, output_dim] # 2\n",
    "\n",
    "# linear layer of shape (feature_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAXCCu9keUlW"
   },
   "source": [
    "### **Other Module Layers**\n",
    "Hay varias capas preconfiguradas en el módulo `nn`. Algunos ejemplos comunes son `nn.Conv2d`, `nn.ConvTranspose2d`, `nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.Upsample` y `nn.MaxPool2d`, entre muchos otros. Aprenderemos más sobre estos a medida que avancemos en el curso. Por ahora, lo único importante a recordar es que podemos tratar cada una de estas capas como componentes plug and play: proporcionaremos las dimensiones requeridas y `PyTorch` se encargará de configurarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yslDOK66fYWn"
   },
   "source": [
    "### **Activation Function Layer**\n",
    "We can also use the `nn` module to apply activations functions to our tensors. Activation functions are used to add non-linearity to our network. Some examples of activations functions are `nn.ReLU()`, `nn.Sigmoid()` and `nn.LeakyReLU()`. Activation functions operate on each element seperately, so the shape of the tensors we get as an output are the same as the ones we pass in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrJP5CveeOON",
    "outputId": "b899dc1f-b2d8-43f5-8338-c0606271e3b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8810,  0.3769],\n",
       "         [-0.8810,  0.3769],\n",
       "         [-0.8810,  0.3769]],\n",
       "\n",
       "        [[-0.8810,  0.3769],\n",
       "         [-0.8810,  0.3769],\n",
       "         [-0.8810,  0.3769]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9v5FjQtd4Ck",
    "outputId": "2cc3cdf3-8153-4a43-d642-420440956523"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2930, 0.5931],\n",
       "         [0.2930, 0.5931],\n",
       "         [0.2930, 0.5931]],\n",
       "\n",
       "        [[0.2930, 0.5931],\n",
       "         [0.2930, 0.5931],\n",
       "         [0.2930, 0.5931]]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(linear_output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiYTthJwhEYT"
   },
   "source": [
    "### **Poniéndolo todo junto**\n",
    "Hasta ahora hemos visto que podemos crear capas y pasar la salida de una como entrada de la siguiente. En lugar de crear tensores intermedios y pasarlos, podemos usar `nn.Sequentual`, que hace exactamente eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtJeOqLxhBLY",
    "outputId": "054d47ee-ae85-4cc7-8cd7-07e446232f84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6822, 0.5056],\n",
       "         [0.6822, 0.5056],\n",
       "         [0.6822, 0.5056]],\n",
       "\n",
       "        [[0.6822, 0.5056],\n",
       "         [0.6822, 0.5056],\n",
       "         [0.6822, 0.5056]]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = nn.Sequential(\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "input = torch.ones(2,3,4)\n",
    "output = block(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkJ81p3GUVPM"
   },
   "source": [
    "### Custom Modules\n",
    "\n",
    "En vez de usar modulos predefinidos, podemos construir los nuestros extendiendo la clase `nn.Module`. Por ejemplo, podemos construir un `nn.Linear` (que también extiende `nn.Module`) usando el tensor introducido anteriormente! También podemos construir nuevos modulos más complejos, como una red neuronal personalizada. Practicarás esto en la siguiente tarea.\n",
    "\n",
    "Para crear un módulo personalizado, lo primero que debemos hacer es extender `nn.Module`. Luego podemos inicializar nuestros parámetros en la función `__init__`, comenzando con una llamada a la función `__init__` de la superclase. Todos los atributos de clase que definimos que son objetos de módulo `nn` se tratan como parámetros, que se pueden aprender durante el entrenamiento. Los tensores no son parámetros, pero se pueden convertir en parámetros si se envuelven en la clase `nn.Parameter`.\n",
    "\n",
    "Todos los módulos que extienden `nn.Module` también deben implementar una función `forward(x)`, donde `x` es un tensor. Esta es la función que se llama cuando se pasa un parámetro a nuestro módulo, como en `model(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2P7eZiMj32_"
   },
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    # Call to the __init__ function of the super class\n",
    "    super(MultilayerPerceptron, self).__init__()\n",
    "\n",
    "    # Bookkeeping: Saving the initialization parameters\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    # Defining of our model\n",
    "    # There isn't anything specific about the naming of `self.model`. It could\n",
    "    # be something arbitrary.\n",
    "    self.model = nn.Sequential(\n",
    "        nn.Linear(self.input_size, self.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.hidden_size, self.input_size),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.model(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2DrfLiBVjNT"
   },
   "source": [
    "Aquí hay una forma alternativa de definir la misma clase. Puedes ver que podemos reemplazar `nn.Sequential` definiendo las capas individuales en el método `__init__` y conectándolas en el método `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-lqhsqwViIk"
   },
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    # Call to the __init__ function of the super class\n",
    "    super(MultilayerPerceptron, self).__init__()\n",
    "\n",
    "    # Bookkeeping: Saving the initialization parameters\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    # Defining of our layers\n",
    "    self.linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.linear2 = nn.Linear(self.hidden_size, self.input_size)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    linear = self.linear(x)\n",
    "    relu = self.relu(linear)\n",
    "    linear2 = self.linear2(relu)\n",
    "    output = self.sigmoid(linear2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQelcFo5bXgU"
   },
   "source": [
    "Ahora que hemos definido \n",
    "Now that we have defined our class, we can instantiate it and see what it does.hor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXi0T0FZbV0y",
    "outputId": "7add78d5-712f-41e2-fd65-d9149331c020"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4621, 0.5310, 0.4358, 0.6027, 0.6664],\n",
       "        [0.4217, 0.5628, 0.3208, 0.5732, 0.6326]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a sample input\n",
    "input = torch.randn(2, 5)\n",
    "\n",
    "# Create our model\n",
    "model = MultilayerPerceptron(5, 3)\n",
    "\n",
    "# Pass our input through our model\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCCbjc-Fb2-B"
   },
   "source": [
    "Podemos inspeccionar los parámetros de nuestro modelo con los métodos `named_parameters()` y `parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d23soYIb2WZ",
    "outputId": "b520a8fc-3911-495c-cd48-49f1ade47beb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('linear.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0359,  0.0409, -0.2182,  0.0993,  0.0216],\n",
       "          [ 0.3572, -0.0213,  0.0229, -0.1651,  0.3042],\n",
       "          [ 0.1404,  0.1992, -0.2556, -0.1605,  0.0395]], requires_grad=True)),\n",
       " ('linear.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1759, -0.1290,  0.0760], requires_grad=True)),\n",
       " ('linear2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.2302,  0.0376, -0.4018],\n",
       "          [-0.4662, -0.4792,  0.1319],\n",
       "          [-0.5094,  0.3908, -0.1104],\n",
       "          [ 0.3688,  0.1750, -0.3135],\n",
       "          [ 0.3631,  0.3253, -0.2241]], requires_grad=True)),\n",
       " ('linear2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.1755,  0.3826, -0.4531,  0.3189,  0.5156], requires_grad=True))]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5JegycOdMFy"
   },
   "source": [
    "## Optimization\n",
    "Hemos mostrado como se calculan los gradientes con la función `backward()`. Tener los gradientes no es suficiente para que nuestros modelos aprendan. También necesitamos saber cómo actualizar los parámetros de nuestros modelos. Aquí es donde entran los optimizadores. El módulo `torch.optim` contiene varios optimizadores que podemos usar. Algunos ejemplos populares son `optim.SGD` y `optim.Adam`. Al inicializar optimizadores, pasamos los parámetros de nuestro modelo, que se pueden acceder con `model.parameters()`, indicando a los optimizadores qué valores optimizarán. Los optimizadores también tienen un parámetro de tasa de aprendizaje (`lr`), que determina cuán grande será la actualización en cada paso. Los diferentes optimizadores tienen diferentes hiperparámetros también.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0F-TvV0kk-I"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgak6o5dlQWF"
   },
   "source": [
    "Después de definir nuestra función de optimización, podemos definir una `loss` que queremos optimizar. Podemos definir la pérdida nosotros mismos, o usar una de las funciones de pérdida predefinidas en `PyTorch`, como `nn.BCELoss()`. ¡Pongamos todo junto ahora! Empezaremos creando algunos datos ficticios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGYFiaT_vXBn",
    "outputId": "4f4a1f82-f708-4f8b-c5c2-5232cbbe8615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4604,  2.1893,  0.8026,  0.6062,  0.9302],\n",
       "        [ 0.2317,  1.1250,  1.1276,  0.3870,  2.0563],\n",
       "        [ 0.3060,  2.9154,  0.0979,  0.8160,  1.9484],\n",
       "        [ 0.2887,  0.1449,  0.1708, -0.0307,  2.0472],\n",
       "        [ 0.6013,  1.8131,  0.7032,  0.2523, -0.0232],\n",
       "        [ 2.6874,  0.5569,  0.4187,  1.2088,  0.2832],\n",
       "        [ 2.8989,  1.5482,  0.3915,  0.5046,  0.1376],\n",
       "        [ 1.1544,  0.3065, -0.2656,  2.5511,  0.2247],\n",
       "        [ 1.7423, -0.1496,  2.0271,  1.0739,  1.1934],\n",
       "        [ 0.3968,  0.2009,  0.6164,  1.8774,  1.1588]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the y data\n",
    "y = torch.ones(10, 5)\n",
    "\n",
    "# Add some noise to our goal y to generate our x\n",
    "# We want out model to predict our original data, albeit the noise\n",
    "x = y + torch.randn_like(y)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEsiOdpWvfLj"
   },
   "source": [
    "Ahora, podemos definir nuestro modelo, optimizador y la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2oA2XsdsbN8p",
    "outputId": "0c1235ec-9df4-4d58-ce75-8c5d123ac40f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18392038345336914"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = MultilayerPerceptron(5, 3)\n",
    "\n",
    "# Define the optimizer\n",
    "adam = optim.Adam(model.parameters(), lr=1e-1)\n",
    "\n",
    "# Define loss using a predefined loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Calculate how our model is doing now\n",
    "y_pred = model(x)\n",
    "loss_function(y_pred, y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXmyqSwmWfNu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtxU7Y8ZufSR"
   },
   "source": [
    "Veamos si  podemos lograr que nuestro modelo logre una pérdida más pequeña. Ahora que tenemos todo lo que necesitamos, podemos configurar nuestro bucle de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogl6-Ctmuek6",
    "outputId": "b860ab08-f6a0-4f04-f9d0-19f4a8df595b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: traing loss: 0.18392038345336914\n",
      "Epoch 1: traing loss: 0.1214178055524826\n",
      "Epoch 2: traing loss: 0.0665205642580986\n",
      "Epoch 3: traing loss: 0.031827643513679504\n",
      "Epoch 4: traing loss: 0.012266852892935276\n",
      "Epoch 5: traing loss: 0.003626907477155328\n",
      "Epoch 6: traing loss: 0.000923578510992229\n",
      "Epoch 7: traing loss: 0.00023057256476022303\n",
      "Epoch 8: traing loss: 6.007583942846395e-05\n",
      "Epoch 9: traing loss: 1.6709067494957708e-05\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epoch, which determines the number of training iterations\n",
    "n_epoch = 10\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "  # Set the gradients to 0\n",
    "  adam.zero_grad()\n",
    "\n",
    "  # Get the model predictions\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Get the loss\n",
    "  loss = loss_function(y_pred, y)\n",
    "\n",
    "  # Print stats\n",
    "  print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
    "\n",
    "  # Compute the gradients\n",
    "  loss.backward()\n",
    "\n",
    "  # Take a step to optimize the weights\n",
    "  adam.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrMJ8AmqeCY-",
    "outputId": "3e70c28c-5f01-45ef-fd27-c2fcf428698d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3774, -0.2663, -0.2254, -0.3110, -0.0272],\n",
       "         [ 1.1547,  0.4672,  0.5807,  0.3853,  0.7197],\n",
       "         [ 0.5740,  1.1962,  0.7437,  0.9969,  1.0613]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0332, 0.8876, 0.4750], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.3789,  0.8695,  0.9322],\n",
       "         [-0.0926,  0.9584,  0.9451],\n",
       "         [ 0.1741,  0.5681,  0.5995],\n",
       "         [-0.5748,  1.1098,  0.6488],\n",
       "         [-0.0048,  1.2007,  1.2029]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.8713, 1.1602, 0.9818, 0.8432, 0.4917], requires_grad=True)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nXApd82wlsF"
   },
   "source": [
    "Puedes ver que la pérdida decrece. Veamos las predicciones de nuestro modelo y veamos si están cerca de nuestro `y` original, que era todo `1s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRqE7P9EtvuS",
    "outputId": "2b2db865-2bb2-4598-85ee-625b9fab7ef9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.9994, 1.0000, 1.0000],\n",
       "        [0.9999, 1.0000, 0.9984, 0.9998, 1.0000],\n",
       "        [1.0000, 1.0000, 0.9996, 1.0000, 1.0000],\n",
       "        [0.9980, 0.9989, 0.9884, 0.9975, 0.9995],\n",
       "        [0.9990, 0.9994, 0.9925, 0.9985, 0.9998],\n",
       "        [0.9999, 1.0000, 0.9987, 0.9999, 1.0000],\n",
       "        [1.0000, 1.0000, 0.9991, 1.0000, 1.0000],\n",
       "        [0.9995, 0.9997, 0.9952, 0.9993, 0.9999],\n",
       "        [1.0000, 1.0000, 0.9991, 1.0000, 1.0000],\n",
       "        [0.9997, 0.9998, 0.9963, 0.9994, 1.0000]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how our model performs on the training data\n",
    "y_pred = model(x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJng31_Pi2R6",
    "outputId": "92a56e62-3e8b-4a45-b6d0-b7581bb6ce6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.9999, 1.0000, 1.0000],\n",
       "        [0.9973, 0.9985, 0.9859, 0.9976, 0.9993],\n",
       "        [1.0000, 1.0000, 0.9997, 1.0000, 1.0000],\n",
       "        [0.9997, 0.9998, 0.9966, 0.9991, 1.0000],\n",
       "        [1.0000, 1.0000, 0.9999, 1.0000, 1.0000],\n",
       "        [0.9983, 0.9990, 0.9893, 0.9971, 0.9996],\n",
       "        [1.0000, 1.0000, 0.9993, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 0.9994, 1.0000, 1.0000],\n",
       "        [0.9999, 1.0000, 0.9988, 0.9999, 1.0000],\n",
       "        [0.9999, 0.9999, 0.9979, 0.9998, 1.0000]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create test data and check how our model performs on it\n",
    "x2 = y + torch.randn_like(y)\n",
    "y_pred = model(x2)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WNk6oIZw2xo"
   },
   "source": [
    "Great! Looks like our model almost perfectly learned to filter out the noise from the `x` that we passed in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8rUNk_1xG1v"
   },
   "source": [
    "## Demo: Word Window Classification\n",
    "\n",
    "Hasta ahora, hemos aprendido los fundamentos de PyTorch y construido una red básica que resuelve una simple tarea. Ahora intentaremos resolver una tarea de NLP. Aquí están las cosas que aprenderemos:\n",
    "\n",
    "1. Datos: crear un Dataset de tensores en batch\n",
    "2. Modelado\n",
    "3. Entrenamiento\n",
    "4. Predicción\n",
    "\n",
    "En esta sección, nuestro objetivo es entrenar un modelo que encuentre las palabras en una oración que correspondan a una `LOCATION`, que siempre será de longitud `1` (lo que significa que `San Fransisco` no se reconocerá como una `LOCATION`). Nuestra tarea se llama `Word Window Classification` por una razón. En lugar de permitir que nuestro modelo solo mire una palabra en cada paso hacia adelante, nos gustaría que pueda considerar el contexto de la palabra en cuestión. Es decir, para cada palabra, queremos que nuestro modelo sea consciente de las palabras circundantes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_amzuUx8BJXI"
   },
   "source": [
    "### Data\n",
    "\n",
    "Preparemos el dataset. \n",
    "The very first task of any machine learning project is to set up our training set. Usually, there will be a training corpus we will be utilizing. In NLP tasks, the corpus would generally be a `.txt` or `.csv` file where each row corresponds to a sentence or a tabular datapoint. In our toy task, we will assume that we have already read our data and the corresponding labels into a `Python` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDiI1PLMw10z"
   },
   "outputs": [],
   "source": [
    "# Our raw data, which consists of sentences\n",
    "corpus = [\n",
    "          \"We always come to Paris\",\n",
    "          \"The professor is from Australia\",\n",
    "          \"I live in Stanford\",\n",
    "          \"He comes from Taiwan\",\n",
    "          \"The capital of Turkey is Ankara\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t33Uke9AE22s"
   },
   "source": [
    "#### Preprocesado\n",
    "\n",
    "Para hacer nuestra tarea más facil, aplicamos algunas etapas de preprocesamiento a nuestros datos. Esto es especialmente importante cuando se trata de datos de texto. Aquí hay algunos ejemplos de preprocesamiento de texto:\n",
    "* **Tokenization**: Tokenizar las oraciones en palabras. \n",
    "* **Lowercasing**: Cambiar todas las letras a minúsculas. \n",
    "* **Noise removal:** Eliminar caracteres especiales tales como la puntuación.\n",
    "* **Stop words removal**: Eliminar palabras usadas comúnmente.\n",
    "\n",
    "Los paso específicos de preprocesamiento que necesitamos dependerán de la tarea que estamos realizando. Por ejemplo, aunque es útil eliminar caracteres especiales en algunas tareas, para otras pueden ser importantes (por ejemplo, si estamos tratando con varios idiomas). Para nuestra tarea, convertiremos nuestras palabras en minúsculas y tokenizaremos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTGn8ANTzZXT"
   },
   "outputs": [],
   "source": [
    "# The preprocessing function we will use to generate our training examples\n",
    "# Our function is a simple one, we lowercase the letters\n",
    "# and then tokenize the words.\n",
    "def preprocess_sentence(sentence):\n",
    "  return sentence.lower().split()\n",
    "\n",
    "# Create our training set\n",
    "train_sentences = [preprocess_sentence(sent) for sent in corpus]\n",
    "train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4jzo5tp0Hza"
   },
   "source": [
    "For each training example we have, we should also have a corresponding label. Recall that the goal of our model was to determine which words correspond to a `LOCATION`. That is, we want our model to output `0` for all the words that are not `LOCATION`s and `1` for the ones that are `LOCATION`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wo1kcMAHFw7"
   },
   "outputs": [],
   "source": [
    "# Set of locations that appear in our corpus\n",
    "locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n",
    "\n",
    "# Our train labels\n",
    "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgVKH9M3RtPx"
   },
   "source": [
    "#### Convirtiendo palabras a embeddings\n",
    "\n",
    "Veamos nuestros datos de entrenamiento un poco más de cerca. Cada dato que tenemos es una secuencia de palabras. Por otro lado, sabemos que los modelos de aprendizaje automático trabajan con números en vectores. ¿Cómo vamos a convertir palabras en números? Puede que estés pensando en embeddings y tienes razón!\n",
    "\n",
    "Imagina que tenemos una tabla de búsqueda de embeddings `E`, donde cada fila corresponde a un embedding. Es decir, cada palabra en nuestro vocabulario tendría una fila de embedding correspondiente `i` en esta tabla. Siempre que queramos encontrar un embedding para una palabra, seguiremos estos pasos:\n",
    "1. Encontrar el índice correspondiente `i` de la palabra en la tabla de embedding: `word->index`.\n",
    "2. Indexar en la tabla de embedding y obtener el embedding: `index->embedding`.\n",
    "\n",
    "Veamos el primer paso. Debemnos asignar todas las palabras en nuestro vocabulario a un índice correspondiente. Podemos hacerlo de la siguiente manera:\n",
    "1. Encontrar todas las palabras únicas en nuestro corpus.\n",
    "2. Asignar un índice a cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjTDlfPyVp5z"
   },
   "outputs": [],
   "source": [
    "# Find all the unique words in our corpus\n",
    "vocabulary = set(w for s in train_sentences for w in s)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOxnKznOWXSC"
   },
   "source": [
    "`vocabulary`  ahora contiene todas las palabras en nuestro corpus. Por otro lado, durante el tiempo de prueba, podemos ver palabras que no están contenidas en nuestro vocabulario. Si podemos encontrar una manera de representar las palabras desconocidas, nuestro modelo aún puede razonar sobre si son una `LOCATION`, ya que también estamos viendo las palabras vecinas para cada predicción.\n",
    "\n",
    "Introducimos un token especial, `<unk>`, para abordar las palabras que están fuera del vocabulario. Podríamos elegir otra cadena para nuestro token desconocido si quisiéramos. El único requisito aquí es que nuestro token debe ser único: solo deberíamos estar usando este token para palabras desconocidas. También agregaremos este token especial a nuestro vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygxYuE1DYeR3"
   },
   "outputs": [],
   "source": [
    "# Add the unknown token to our vocabulary\n",
    "vocabulary.add(\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsf4haL94AFu"
   },
   "source": [
    "Antes mencionames que nuestra tarea se llama `Word Window Classification` porque nuestro modelo está mirando las palabras circundantes además de la palabra dada cuando necesita hacer una predicción.\n",
    "\n",
    "Por ejemplo consideremos la oración \"Siempre venimos a París\". La etiqueta de entrenamiento correspondiente para esta oración es `0, 0, 0, 0, 1` ya que solo París, la última palabra, es una `LOCATION`. En un pase (lo que significa una llamada a `forward()`), nuestro modelo intentará generar la etiqueta correcta para una palabra. Digamos que nuestro modelo está tratando de generar la etiqueta correcta `1` para `París`. Si solo permitimos que nuestro modelo vea `París`, pero nada más, perderemos la información importante de que la palabra a menudo aparece con LOCATION`s.\n",
    "\n",
    "Word windows permite a nuestro modelo considerar las `N` palabras circundantes de cada palabra al hacer una predicción. En nuestro ejemplo anterior para `París`, si tenemos un tamaño de ventana de `1`, eso significa que nuestro modelo mirará las palabras que vienen inmediatamente antes y después de `París`, que son `a` y bueno, nada. Ahora, esto plantea otro problema. `París` está al final de nuestra oración, por lo que no hay otra palabra que la siga. Recuerda que definimos las dimensiones de entrada de nuestros modelos de `PyTorch` cuando los inicializamos. Si configuramos el tamaño de la ventana en `1`, significa que nuestro modelo aceptará `3` palabras en cada paso. No podemos hacer que nuestro modelo espere `2` palabras de vez en cuando.\n",
    "\n",
    "\n",
    "La solución es introducir un token especial, como `<pad>`, que se agregará a nuestras oraciones para asegurarnos de que cada palabra tenga una ventana válida a su alrededor. Similar al token `<unk>`, podríamos elegir otra cadena para nuestro token de relleno si quisiéramos, siempre y cuando nos aseguremos de que se use para un propósito único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVQsjYi6ZegI"
   },
   "outputs": [],
   "source": [
    "# Add the <pad> token to our vocabulary\n",
    "vocabulary.add(\"<pad>\")\n",
    "\n",
    "# Function that pads the given sentence\n",
    "# We are introducing this function here as an example\n",
    "# We will be utilizing it later in the tutorial\n",
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "  window = [pad_token] * window_size\n",
    "  return window + sentence + window\n",
    "\n",
    "# Show padding example\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqvgWKwSNpAd"
   },
   "source": "Ahora que tenemos nuestro vocabulario, asignemos un indice a cada una de nuestras palabras."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNCTQnKDa4oh"
   },
   "outputs": [],
   "source": [
    "# We are just converting our vocabulary to a list to be able to index into it\n",
    "# Sorting is not necessary, we sort to show an ordered word_to_ind dictionary\n",
    "# That being said, we will see that having the index for the padding token\n",
    "# be 0 is convenient as some PyTorch functions use it as a default value\n",
    "# such as nn.utils.rnn.pad_sequence, which we will cover in a bit\n",
    "ix_to_word = sorted(list(vocabulary))\n",
    "\n",
    "# Creating a dictionary to find the index of a given word\n",
    "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pt-0SK67hMVo"
   },
   "outputs": [],
   "source": [
    "ix_to_word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELZuteqbdWd1"
   },
   "source": "Listo! Ahora podemos convertir nuestras oraciones en secuencia de indices que corresponden a cada token. "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNOxip15bMfH"
   },
   "outputs": [],
   "source": [
    "# Given a sentence of tokens, return the corresponding indices\n",
    "def convert_token_to_indices(sentence, word_to_ix):\n",
    "  indices = []\n",
    "  for token in sentence:\n",
    "    # Check if the token is in our vocabularly. If it is, get it's index.\n",
    "    # If not, get the index for the unknown token.\n",
    "    if token in word_to_ix:\n",
    "      index = word_to_ix[token]\n",
    "    else:\n",
    "      index = word_to_ix[\"<unk>\"]\n",
    "    indices.append(index)\n",
    "  return indices\n",
    "\n",
    "# More compact version of the same function\n",
    "def _convert_token_to_indices(sentence, word_to_ix):\n",
    "  return [word_to_ind.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "\n",
    "# Show an example\n",
    "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "example_indices = convert_token_to_indices(example_sentence, word_to_ix)\n",
    "restored_example = [ix_to_word[ind] for ind in example_indices]\n",
    "\n",
    "print(f\"Original sentence is: {example_sentence}\")\n",
    "print(f\"Going from words to indices: {example_indices}\")\n",
    "print(f\"Going from indices to words: {restored_example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jsXw8cB1xpH"
   },
   "source": [
    "\n",
    "En el ejemplo de arriba, `kuwait` aparece como `<unk>`, porque no está incluido en nuestro vocabulario. Convirtamos nuestras `train_sentences` a `example_padded_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRaKQwSJH-1d"
   },
   "outputs": [],
   "source": [
    "# Converting our sentences to indices\n",
    "example_padded_indices = [convert_token_to_indices(s, word_to_ix) for s in train_sentences]\n",
    "example_padded_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZULjHBjHsEK"
   },
   "source": [
    "Ahora que tenemos un índice para cada palabra de nuestro vocabulario, podemos crear una embedding table con la clase `nn.Embedding` en `PyTorch`. Se llama de la siguiente manera `nn.Embedding(num_words, embedding_dimension)` donde `num_words` es el número de palabras en nuestro vocabulario y `embedding_dimension` es la dimensión de los embeddings que queremos tener. No hay nada especial en `nn.Embedding`: es solo una clase envoltorio alrededor de un tensor de dimensión `NxE` entrenable, donde `N` es el número de palabras en nuestro vocabulario y `E` es el número de dimensiones de embedding. Esta tabla es inicialmente aleatoria, pero cambiará con el tiempo. A medida que entrenamos nuestra red, los gradientes se propagarán hacia atrás hasta la capa de embedding, y por lo tanto nuestros embeddings de palabras se actualizarán. Inicializaremos la capa de embedding que usaremos para nuestro modelo en nuestro modelo, pero estamos mostrando un ejemplo aquí.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4AgHzv91VXx"
   },
   "outputs": [],
   "source": [
    "# Creating an embedding table for our words\n",
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "\n",
    "# Printing the parameters in our embedding table\n",
    "list(embeds.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI7ZTt4OkpPp"
   },
   "source": [
    "Para obtener el embedding de una palabra en nuestro vocabulario, todo lo que necesitamos hacer es crear un tensor de búsqueda. El tensor de búsqueda es solo un tensor que contiene el índice que queremos buscar. La clase `nn.Embedding` espera un tensor de índice que sea de tipo Long Tensor, por lo que debemos crear nuestro tensor en consecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkldmcepjfh_"
   },
   "outputs": [],
   "source": [
    "# Get the embedding for the word Paris\n",
    "index = word_to_ix[\"paris\"]\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUsdwBOxm6B4"
   },
   "outputs": [],
   "source": [
    "# We can also get multiple embeddings at once\n",
    "index_paris = word_to_ix[\"paris\"]\n",
    "index_ankara = word_to_ix[\"ankara\"]\n",
    "indices = [index_paris, index_ankara]\n",
    "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "embeddings = embeds(indices_tensor)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bgSW3LPltkF"
   },
   "source": [
    "Usualmente definimos una embedding layer como parte de nuestro modelo, lo que veremos en las siguientes secciones de nuestro notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHCXeQOamHU1"
   },
   "source": [
    "#### Batching Oraciones\n",
    "\n",
    "Esperar a que el corpus de entrenamiento sea procesado completamente es costoso. Por otro lado, actualizar los parámetros después de cada ejemplo de entrenamiento hace que la pérdida sea menos estable entre actualizaciones. Para combatir estos problemas, actualizamos nuestros parámetros después de entrenar en un batch. Esto nos permite obtener una mejor estimación del gradiente del loss. En esta sección, aprenderemos a estructurar nuestros datos en lotes utilizando la clase `torch.util.data.DataLoader`.\n",
    "\n",
    "Llamaremos a la clase `DataLoader` de la siguiente manera: `DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)`. El parámetro `batch_size` determina el número de samples por batch. En cada época, iteraremos sobre todos los batches utilizando el `DataLoader`. El orden de los batches es determinista por defecto, pero podemos pedirle al `DataLoader` que mezcle los batches configurando el parámetro `shuffle` en `True`. De esta manera, nos aseguramos de que no encontramos un mal batch varias veces.\n",
    "\n",
    "Si se proporciona, `DataLoader` pasa los batches que prepara a `collate_fn`. Podemos escribir una función personalizada para pasar al parámetro `collate_fn` para imprimir estadísticas sobre nuestro batch o realizar un procesamiento adicional. En nuestro caso, usaremos el `collate_fn` para:\n",
    "1. Window pad nuestras oraciones de entrenamiento.\n",
    "2. Convertir las palabras en los ejemplos de entrenamiento a índices.\n",
    "3. Rellenar los ejemplos de entrenamiento para que todas las oraciones y etiquetas tengan la misma longitud. Del mismo modo, también necesitamos rellenar las etiquetas. Esto crea un problema porque al calcular la pérdida, necesitamos saber el número real de palabras en un ejemplo dado. También haremos un seguimiento de este número en la función que pasamos al parámetro `collate_fn`.\n",
    "\n",
    "Ya que nuestra versión de `collate_fn` necesitará acceso a nuestro diccionario `word_to_ix` (para que pueda convertir palabras en índices), haremos uso de la función `partial` en `Python`, que pasa los parámetros que le damos a la función que le pasamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkvvVlo4jgFm"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "def custom_collate_fn(batch, window_size, word_to_ix):\n",
    "  # Break our batch into the training examples (x) and labels (y)\n",
    "  # We are turning our x and y into tensors because nn.utils.rnn.pad_sequence\n",
    "  # method expects tensors. This is also useful since our model will be\n",
    "  # expecting tensor inputs.\n",
    "  x, y = zip(*batch)\n",
    "\n",
    "  # Now we need to window pad our training examples. We have already defined a\n",
    "  # function to handle window padding. We are including it here again so that\n",
    "  # everything is in one place.\n",
    "  def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    window = [pad_token] * window_size\n",
    "    return window + sentence + window\n",
    "\n",
    "  # Pad the train examples.\n",
    "  x = [pad_window(s, window_size=window_size) for s in x]\n",
    "\n",
    "  # Now we need to turn words in our training examples to indices. We are\n",
    "  # copying the function defined earlier for the same reason as above.\n",
    "  def convert_tokens_to_indices(sentence, word_to_ix):\n",
    "    return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "\n",
    "  # Convert the train examples into indices.\n",
    "  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "  # We will now pad the examples so that the lengths of all the example in\n",
    "  # one batch are the same, making it possible to do matrix operations.\n",
    "  # We set the batch_first parameter to True so that the returned matrix has\n",
    "  # the batch as the first dimension.\n",
    "  pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "\n",
    "  # pad_sequence function expects the input to be a tensor, so we turn x into one\n",
    "  x = [torch.LongTensor(x_i) for x_i in x]\n",
    "  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "  # We will also pad the labels. Before doing so, we will record the number\n",
    "  # of labels so that we know how many words existed in each example.\n",
    "  lengths = [len(label) for label in y]\n",
    "  lenghts = torch.LongTensor(lengths)\n",
    "\n",
    "  y = [torch.LongTensor(y_i) for y_i in y]\n",
    "  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "  # We are now ready to return our variables. The order we return our variables\n",
    "  # here will match the order we read them in our training loop.\n",
    "  return x_padded, y_padded, lenghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q0gXea-bCsz"
   },
   "source": [
    "Parece larga esta función, pero no tiene por qué serlo. Aquí esta la alternativa donde eliminamos las declaraciones de funciones y comentarios adicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZfcmAJXbLcq"
   },
   "outputs": [],
   "source": [
    "def _custom_collate_fn(batch, window_size, word_to_ix):\n",
    "  # Prepare the datapoints\n",
    "  x, y = zip(*batch)\n",
    "  x = [pad_window(s, window_size=window_size) for s in x]\n",
    "  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "  # Pad x so that all the examples in the batch have the same size\n",
    "  pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "  x = [torch.LongTensor(x_i) for x_i in x]\n",
    "  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "  # Pad y and record the length\n",
    "  lengths = [len(label) for label in y]\n",
    "  lenghts = torch.LongTensor(lengths)\n",
    "  y = [torch.LongTensor(y_i) for y_i in y]\n",
    "  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "  return x_padded, y_padded, lenghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS1WuQO0Khxx"
   },
   "source": [
    "Ahora podemos ver la `DataLoader` en acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfB0JKL2vZ6p"
   },
   "outputs": [],
   "source": [
    "# Parameters to be passed to the DataLoader\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate the DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Go through one loop\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "  print(f\"Iteration {counter}\")\n",
    "  print(\"Batched Input:\")\n",
    "  print(batched_x)\n",
    "  print(\"Batched Labels:\")\n",
    "  print(batched_y)\n",
    "  print(\"Batched Lengths:\")\n",
    "  print(batched_lengths)\n",
    "  print(\"\")\n",
    "  counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93QOZSsMTFNF"
   },
   "source": [
    "Los tensores de entrada batcheados que ves arriba se pasarán a nuestro modelo. Por otro lado, comenzamos diciendo que nuestro modelo será un Window Classifier. La forma en que se formatean actualmente nuestros tensores de entrada, tenemos todas las palabras en una oración en un datapoint. Cuando pasamos esta entrada a nuestro modelo, necesita crear las ventanas para cada palabra, hacer una predicción sobre si la palabra central es una `LOCATION` o no para cada ventana, juntar las predicciones y devolverlas.\n",
    "\n",
    "Podríamos evitar este problema si formateamos nuestros datos dividiéndolos en ventanas de antemano. En este ejemplo, veremos cómo nuestro modelo se encarga del formateo.\n",
    "\n",
    "Dado que nuestro `window_size` es `N`, queremos que nuestro modelo haga una predicción en cada `2N+1` tokens. Es decir, si tenemos una entrada con `9` tokens y un `window_size` de `2`, queremos que nuestro modelo devuelva `5` predicciones. Esto tiene sentido porque antes de rellenarlo con `2` tokens a cada lado, nuestra entrada también tenía `5` tokens.\n",
    "\n",
    "Podemos crear estas ventanas usando for loops, pero hay una alternativa más rápida en `PyTorch`, que es el método `unfold(dimension, size, step)`. Podemos crear las ventanas que necesitamos de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMZu-pxLVxHQ"
   },
   "outputs": [],
   "source": [
    "# Print the original tensor\n",
    "\n",
    "print(f\"Original Tensor: \")\n",
    "print(batched_x)\n",
    "print(\"\")\n",
    "\n",
    "# Create the 2 * 2 + 1 chunks\n",
    "chunk = batched_x.unfold(1, window_size*2 + 1, 1)\n",
    "print(f\"Windows: \")\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlDbOpeoSKxd"
   },
   "source": [
    "### Modelado\n",
    "\n",
    "Ahora que hemos preparado nuestros datos, estamos listos para construir nuestro modelo. Hemos aprendido a escribir clases `nn.Module` personalizadas. Haremos lo mismo aquí y pondremos todo lo que hemos aprendido hasta ahora juntos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLTU4h76NLYm"
   },
   "outputs": [],
   "source": [
    "class WordWindowClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
    "    super(WordWindowClassifier, self).__init__()\n",
    "\n",
    "    \"\"\" Instance variables \"\"\"\n",
    "    self.window_size = hyperparameters[\"window_size\"]\n",
    "    self.embed_dim = hyperparameters[\"embed_dim\"]\n",
    "    self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "    self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
    "\n",
    "    \"\"\" Embedding Layer\n",
    "    Takes in a tensor containing embedding indices, and returns the\n",
    "    corresponding embeddings. The output is of dim\n",
    "    (number_of_indices * embedding_dim).\n",
    "\n",
    "    If freeze_embeddings is True, set the embedding layer parameters to be\n",
    "    non-trainable. This is useful if we only want the parameters other than the\n",
    "    embeddings parameters to change.\n",
    "\n",
    "    \"\"\"\n",
    "    self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
    "    if self.freeze_embeddings:\n",
    "      self.embeds.weight.requires_grad = False\n",
    "\n",
    "    \"\"\" Hidden Layer\n",
    "    \"\"\"\n",
    "    full_window_size = 2 * window_size + 1\n",
    "    self.hidden_layer = nn.Sequential(\n",
    "      nn.Linear(full_window_size * self.embed_dim, self.hidden_dim),\n",
    "      nn.Tanh()\n",
    "    )\n",
    "\n",
    "    \"\"\" Output Layer\n",
    "    \"\"\"\n",
    "    self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    \"\"\" Probabilities\n",
    "    \"\"\"\n",
    "    self.probabilities = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    \"\"\"\n",
    "    Let B:= batch_size\n",
    "        L:= window-padded sentence length\n",
    "        D:= self.embed_dim\n",
    "        S:= self.window_size\n",
    "        H:= self.hidden_dim\n",
    "\n",
    "    inputs: a (B, L) tensor of token indices\n",
    "    \"\"\"\n",
    "    B, L = inputs.size()\n",
    "\n",
    "    \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L) LongTensor\n",
    "    Outputs a (B, L~, S) LongTensor\n",
    "    \"\"\"\n",
    "    # Fist, get our word windows for each word in our input.\n",
    "    token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
    "    _, adjusted_length, _ = token_windows.size()\n",
    "\n",
    "    # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "    assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
    "\n",
    "    \"\"\"\n",
    "    Embedding.\n",
    "    Takes in a torch.LongTensor of size (B, L~, S)\n",
    "    Outputs a (B, L~, S, D) FloatTensor.\n",
    "    \"\"\"\n",
    "    embedded_windows = self.embeds(token_windows)\n",
    "\n",
    "    \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L~, S, D) FloatTensor.\n",
    "    Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "    -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "    \"\"\"\n",
    "    embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "\n",
    "    \"\"\"\n",
    "    Layer 1.\n",
    "    Takes in a (B, L~, S*D) FloatTensor.\n",
    "    Resizes it into a (B, L~, H) FloatTensor\n",
    "    \"\"\"\n",
    "    layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "    \"\"\"\n",
    "    Layer 2\n",
    "    Takes in a (B, L~, H) FloatTensor.\n",
    "    Resizes it into a (B, L~, 1) FloatTensor.\n",
    "    \"\"\"\n",
    "    output = self.output_layer(layer_1)\n",
    "\n",
    "    \"\"\"\n",
    "    Softmax.\n",
    "    Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.\n",
    "    Outputs a (B, L~, 1) FloatTensor of (log-)normalized class scores.\n",
    "    \"\"\"\n",
    "    output = self.probabilities(output)\n",
    "    output = output.view(B, -1)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avy1fnyAvEcd"
   },
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Pongámolos todo junto. Empecemos preparando nuestros datos y inicializando nuestro modelo. Luego podemos inicializar nuestro optimizador y definir nuestra función de pérdida. Esta vez, en lugar de usar una de las funciones de pérdida predefinidas como hicimos antes, definiremos nuestra propia función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bInu1VqjHsfj"
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate a DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize a model\n",
    "# It is useful to put all the model hyperparameters in a dictionary\n",
    "model_hyperparameters = {\n",
    "    \"batch_size\": 4,\n",
    "    \"window_size\": 2,\n",
    "    \"embed_dim\": 25,\n",
    "    \"hidden_dim\": 25,\n",
    "    \"freeze_embeddings\": False,\n",
    "}\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n",
    "\n",
    "# Define an optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define a loss function, which computes to binary cross entropy loss\n",
    "def loss_function(batch_outputs, batch_labels, batch_lengths):\n",
    "    # Calculate the loss for the whole batch\n",
    "    bceloss = nn.BCELoss()\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "\n",
    "    # Rescale the loss. Remember that we have used lengths to store the\n",
    "    # number of words in each training example\n",
    "    loss = loss / batch_lengths.sum().float()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHxpxDkFHfQE"
   },
   "source": [
    "A diferencia de nuestro ejemplo previo, esta vez en lugar de pasar todos nuestros datos de entrenamiento al modelo de una vez en cada época, utilizaremos lotes. Por lo tanto, en cada iteración de época de entrenamiento, también iteraremos sobre los lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QL9IDgIOvHca"
   },
   "outputs": [],
   "source": [
    "# Function that will be called in every epoch\n",
    "def train_epoch(loss_function, optimizer, model, loader):\n",
    "\n",
    "  # Keep track of the total loss for the batch\n",
    "  total_loss = 0\n",
    "  for batch_inputs, batch_labels, batch_lengths in loader:\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    outputs = model.forward(batch_inputs)\n",
    "    # Compute the batch loss\n",
    "    loss = loss_function(outputs, batch_labels, batch_lengths)\n",
    "    # Calculate the gradients\n",
    "    loss.backward()\n",
    "    # Update the parameteres\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  return total_loss\n",
    "\n",
    "\n",
    "# Function containing our main training loop\n",
    "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
    "\n",
    "  # Iterate through each epoch and call our train_epoch function\n",
    "  for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
    "    if epoch % 100 == 0: print(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjf75cnzJ4n6"
   },
   "source": [
    "Let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kav8kwVBJ6XW"
   },
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-k7Pav4LdQJ"
   },
   "source": [
    "### Predicción\n",
    "\n",
    "Veamos como nuestro modelo se desempeña en un ejemplo de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-v5X69a2Lkbm"
   },
   "outputs": [],
   "source": [
    "# Create test sentences\n",
    "test_corpus = [\"She comes from Paris\"]\n",
    "test_sentences = [s.lower().split() for s in test_corpus]\n",
    "test_labels = [[0, 0, 0, 1]]\n",
    "\n",
    "# Create a test loader\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=False,\n",
    "                                           collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlBa8xaNMZgv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGYn8CAoMTjX"
   },
   "outputs": [],
   "source": [
    "for test_instance, labels, _ in test_loader:\n",
    "  outputs = model.forward(test_instance)\n",
    "  print(labels)\n",
    "  print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iebrU4ZibVIR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
